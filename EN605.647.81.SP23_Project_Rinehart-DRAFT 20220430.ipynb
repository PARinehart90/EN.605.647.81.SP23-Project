{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13af1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0329e3",
   "metadata": {},
   "source": [
    "# Test and Training Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c639aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish Training Set\n",
    "ar_train_x = np.array([[0.90, 0.87], \n",
    "              [1.31, 0.75], \n",
    "              [2.48, 1.14], \n",
    "              [0.41, 1.87], \n",
    "              [2.45, 0.52], \n",
    "              [2.54, 2.97], \n",
    "              [0.07, 0.09],\n",
    "              [1.32, 1.96],\n",
    "              [0.94, 0.34],\n",
    "              [1.75, 2.21],\n",
    "             ])\n",
    "#Training Set Target values\n",
    "ar_train_y = np.array([1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "#Establish Test Set\n",
    "ar_test_x = np.array([[1.81, 1.02], \n",
    "              [2.36, 1.60], \n",
    "              [2.17, 2.08], \n",
    "              [2.85, 2.91], \n",
    "              [1.05, 1.93], \n",
    "              [2.32, 1.73], \n",
    "              [1.86, 1.31],\n",
    "              [1.45, 2.19],\n",
    "              [0.28, 0.71],\n",
    "              [2.49, 1.52],\n",
    "             ])\n",
    "\n",
    "#Test Set Target Values\n",
    "ar_test_y = np.array([0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2719225",
   "metadata": {},
   "source": [
    "# Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48149767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create perceptron class\n",
    "class Perceptron:\n",
    "    \n",
    "    #initialize perceptron class\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.delta_weights = np.zeros_like(weights)\n",
    "        self.bias = bias\n",
    "        self.delta_bias = 0\n",
    "        self.activity = 0\n",
    "        self.activation = 0\n",
    "        self.delta = 0\n",
    "    \n",
    "    #Calculate the activity value\n",
    "    def calc_activity(self, inputs):\n",
    "        self.activity = np.dot(inputs, self.weights) + self.bias\n",
    "        #print(\"activity: \", self.activity)\n",
    "    \n",
    "    #Calculate the activation function given the activity value using sigmoid function\n",
    "    def calc_activation(self, activity):\n",
    "        self.activation = 1/(1+np.exp(-1*self.activity))\n",
    "        #print(\"activation: \", self.activation)\n",
    "    \n",
    "    #Find the change needed in weights and bias\n",
    "    def set_delta_weights(self, inputs, eta, target):\n",
    "        self.delta = self.activation*(1-self.activation)*(target - self.activation)\n",
    "        for i in range(len(inputs)):\n",
    "            self.delta_weights[i] = inputs[i] * self.delta * eta\n",
    "        self.delta_bias =  self.delta * eta\n",
    "        #print(\"Bias: \", self.bias, \" Delta: \", self.delta, \" Eta: \", eta, \" Delta Bias: \", self.delta_bias)\n",
    "    \n",
    "    #update weights and bias\n",
    "    def update_weights(self):\n",
    "        #print(\"Weights: \", self.weights)\n",
    "        #print(self.delta_weights)\n",
    "        #print(\"Bias: \", self.bias)\n",
    "        #print(self.delta_bias)\n",
    "        self.weights += self.delta_weights\n",
    "        self.bias += self.delta_bias\n",
    "        #print(\"Updated bias: \", self.bias)\n",
    "    \n",
    "    #Call necessary functions to update perceptron using perceptron delta function process\n",
    "    def perceptron_delta(self, target, inputs):\n",
    "        self.target = target\n",
    "        self.calc_activity(inputs)\n",
    "        self.calc_activation(self.activity)\n",
    "        self.set_delta_weights(inputs, eta, target)\n",
    "        self.update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9aeff0",
   "metadata": {},
   "source": [
    "# Initialize Single Layer Perceptron and Find Best Weights/Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025bef19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Weight:  [-1.0, -1.0]  Best eta:  1.0 Lowest E:  0.8304399711430172 , when averaged is 0.08304399711430172\n"
     ]
    }
   ],
   "source": [
    "#Define weights, eta values, iterations, and bias\n",
    "weight_vector = [[-1.0, -1.0], [-0.5, -0.5], [0.5, 0.5], [1.0, 1.0]]\n",
    "eta_values = [[0.1], [0.5], [1.0]]\n",
    "iterations = 30\n",
    "bias = 0\n",
    "\n",
    "#Variables to capture best model\n",
    "best_weight = 0\n",
    "best_eta = 0\n",
    "lowest_E = float('inf')\n",
    "\n",
    "#for each set of weights\n",
    "for g in range(0, len(weight_vector)):\n",
    "    \n",
    "    #for each eta value\n",
    "    for h in range(0, len(eta_values)):\n",
    "        #set eta\n",
    "        eta = eta_values[h][0]\n",
    "        \n",
    "        #Create a Perceptron object\n",
    "        Percep = Perceptron(weight_vector[g], bias)\n",
    "        \n",
    "        #train the network for specified number of iterations\n",
    "        for i in range(0,iterations):\n",
    "            total_error = 0\n",
    "            \n",
    "            #for every input pair\n",
    "            for j in range (0, len(ar_train_x)):\n",
    "                #Update the perceptron weights and bias based on the training output value\n",
    "                Percep.perceptron_delta(ar_train_y[j], ar_train_x[j])\n",
    "                \n",
    "                #if final iteration, collect  total error\n",
    "                #if i== iterations-1:\n",
    "                    #total_error = total_error + Percep.delta\n",
    "                total_error = total_error + (0.5)*(ar_train_y[j]-Percep.activation)**2\n",
    "            #print(\"Average Error for iteration:\", i+1,\" is \", round(abs(total_error/len(ar_train_x)),3))\n",
    "            \n",
    "            #print(\"Starting Weights: \", weight_vector[g], \" Eta: \", eta, \" Total Error: \", total_error )\n",
    "            if(abs(total_error) < lowest_E):\n",
    "                lowest_E=total_error\n",
    "                best_weights = weight_vector[g]\n",
    "\n",
    "                best_eta = eta\n",
    "            #if i==29:\n",
    "                #print(\"Start Weights\", weight_vector[g], \"Final Weights: \",Percep.weights, \"Bias: \", Percep.bias, \" Avg Error: \", total_error/10)\n",
    "\n",
    "print(\"Best Weight: \", best_weights, \" Best eta: \", best_eta, \"Lowest E: \", lowest_E, \", when averaged is\", lowest_E/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838f6a3",
   "metadata": {},
   "source": [
    "# Initialize Single-Layer Perceptron with Best Weights/Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51da7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set weights and eta to best model\n",
    "eta = best_eta\n",
    "weight_vector = best_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e79d9d",
   "metadata": {},
   "source": [
    "## Retrain perceptron using Method 1 (30 iters, sequential I/O pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25726cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train perceptron with best model weights and eta\n",
    "Percep = Perceptron(best_weights, bias)\n",
    "for i in range(0,iterations):            \n",
    "        #for every input pair\n",
    "        for j in range (0, len(ar_train_x)):\n",
    "            #Update the perceptron weights and bias based on the training output value\n",
    "            Percep.perceptron_delta(ar_train_y[j], ar_train_x[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb4a1f",
   "metadata": {},
   "source": [
    "## Using trained model, make predictions on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a35900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  0.582  Actual: 1.0\n",
      "Predicted:  0.502  Actual: 1.0\n",
      "Predicted:  0.072  Actual: 0.0\n",
      "Predicted:  0.231  Actual: 0.0\n",
      "Predicted:  0.246  Actual: 0.0\n",
      "Predicted:  0.001  Actual: 1.0\n",
      "Predicted:  0.964  Actual: 1.0\n",
      "Predicted:  0.062  Actual: 0.0\n",
      "Predicted:  0.812  Actual: 1.0\n",
      "Predicted:  0.02  Actual: 0.0\n"
     ]
    }
   ],
   "source": [
    "#With trained model, use training value predictions to find optimal threshold:\n",
    "predictions = []\n",
    "for j in range (0, len(ar_train_x)): \n",
    "    Percep.calc_activation(Percep.calc_activity(ar_train_x[j]))\n",
    "    pred = Percep.activation\n",
    "    predictions.append(pred)\n",
    "    print(\"Predicted: \", round(pred,3), \" Actual:\", ar_train_y[j]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35032e",
   "metadata": {},
   "source": [
    "## Find optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269336e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 90.0  when threshold is:  0.26\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0\n",
    "best_accuracy = 0\n",
    "best_threshold = 0.0\n",
    "\n",
    "while threshold < 1.0:\n",
    "    mapped_predictions = [0 if val < threshold else 1 for val in predictions]\n",
    "    \n",
    "    \n",
    "    # First, calculate the number of correct predictions\n",
    "    correct_predictions = sum([1 for a, b in zip(ar_train_y, mapped_predictions) if a == b])\n",
    "\n",
    "    # Then, calculate the accuracy as a percentage of correct predictions\n",
    "    accuracy = correct_predictions / len(ar_train_y) * 100\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "        \n",
    "    threshold = threshold + 0.02\n",
    "\n",
    "print(\"Best Accuracy:\", best_accuracy, \" when threshold is: \", round(best_threshold,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e16709d",
   "metadata": {},
   "source": [
    "## Apply threshold and evaluate against Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccda5e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "#With trained model, use test values to evaluate\n",
    "for j in range (0, len(ar_test_x)): \n",
    "    Percep.calc_activation(Percep.calc_activity(ar_test_x[j]))\n",
    "    pred = Percep.activation\n",
    "    predictions.append(pred)\n",
    "    #print(\"Predicted: \", round(pred,3), \" Actual:\", ar_test_y[j]) \n",
    "    \n",
    "mapped_predictions = [0 if val < best_threshold else 1 for val in predictions]\n",
    "correct_predictions = sum([1 for a, b in zip(ar_test_y, mapped_predictions) if a == b])\n",
    "accuracy = correct_predictions / len(ar_test_y) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe07f8",
   "metadata": {},
   "source": [
    "## A threshold of 0.26 would correctly match 80% of Test and 90% Train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649df4ac",
   "metadata": {},
   "source": [
    "# Multi-Layer Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f017c0f",
   "metadata": {},
   "source": [
    "## Define layer class that will be used for a 2 layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080d206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    #initialize layer class\n",
    "    def __init__(self, layer_weight, eta = 0,  bias = None):\n",
    "        self.layer_weight = layer_weight\n",
    "        self.layer_bias = bias\n",
    "        self.eta = eta\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def get_layer_output_vector(self, inputs):\n",
    "        self.activity = np.dot(inputs ,self.layer_weight)+self.layer_bias\n",
    "        self.output_vector = 1/(1+np.exp(-self.activity))\n",
    "        return self.output_vector\n",
    "    \n",
    "    def get_error_vector(self, desired_output):\n",
    "        self.littleE_vector = desired_output - self.output_vector\n",
    "        return self.littleE_vector\n",
    "    \n",
    "    def calc_delta_k(self, target):\n",
    "        #print(\"Target is \", target)\n",
    "        #print(\"Output vector is: \", self.output_vector)\n",
    "        self.littleE = target - self.output_vector\n",
    "        #print(\"Little E is \", self.littleE)\n",
    "        self.bigE = 0.5*self.littleE**2\n",
    "        self.delta_k = self.littleE*(1-self.output_vector)*self.output_vector\n",
    "        return self.delta_k\n",
    "    \n",
    "    def calc_delta_j(self, delta_k, next_layer_weights):\n",
    "        #print(delta_k)\n",
    "        #print(next_layer_weights)\n",
    "        self.delta_j = (1-self.output_vector)*self.output_vector*(delta_k*next_layer_weights).T\n",
    "        return self.delta_j\n",
    "    \n",
    "    def calc_delta_weights (self, delta, prev_layer_input):\n",
    "        #print(\"---DELTA WEIGHT CALC----\")\n",
    "        #print(\"Self Weights\", self.layer_weight)\n",
    "        #print(\"shape \", self.layer_weight.shape)\n",
    "        #print(\"eta\", self.eta)\n",
    "        #print(\"delta\", delta)\n",
    "        #print(\"Prev Layer input \", prev_layer_input)\n",
    "        self.delta_weights = self.layer_weight + self.eta*delta*prev_layer_input\n",
    "        self.delta_bias = self.layer_bias + self.eta*delta\n",
    "        return self.delta_weights, self.delta_bias\n",
    "    \n",
    "    def update_layer_weights(self):\n",
    "        self.layer_weight = self.delta_weights\n",
    "        self.layer_bias = self.delta_bias\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c3f2b",
   "metadata": {},
   "source": [
    "# Initialize Multi-Layer Network and Find Best Weights/Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98388f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Weight for layer 1:  [[-1. -1.]\n",
      " [-1. -1.]] Best weights for layer 2: [1. 1.]  Best eta:  0.6555333371303738 Lowest E:  [[0.9438601]] , when averaged is  [[0.09438601]]\n"
     ]
    }
   ],
   "source": [
    "#Define weights, eta values, iterations, and bias\n",
    "weight_layer_1 = np.array([[[-1.0, -1.0], [-1.0, -1.0]], [[-0.5, -0.5], [-0.5, -.05]], [[0.5, 0.5], [0.5, 0.5]], [[1.0, 1.0], [1.0, 1.0]]])\n",
    "weight_layer_2 = np.array([[-1.0, -1.0], [-0.5, -0.5], [0.5, 0.5], [1.0, 1.0]])\n",
    "\n",
    "eta_values = [[random.uniform(0,1)], [random.uniform(0,1)], [random.uniform(0,1)]]\n",
    "iterations = 30\n",
    "bias = 0\n",
    "\n",
    "#Variables to capture best model\n",
    "best_weights1 = 0\n",
    "best_weights2 = 0\n",
    "best_eta = 0\n",
    "lowest_E = float('inf')\n",
    "\n",
    "#for each set of weights in layer 1\n",
    "for f in range(0, len(weight_layer_1)):\n",
    "    #Layer1=Layer(weight_layer_1[f].reshape(2,2), eta, bias)\n",
    "    #print(\"Layer 1 weights: \", Layer1.layer_weight)\n",
    "    \n",
    "    #for each set of weights in layer 2\n",
    "    for g in range (0, len(weight_layer_2)):\n",
    "        \n",
    "        #for each eta value in our list\n",
    "        for h in range(0, len(eta_values)):\n",
    "            #set eta, layer weights\n",
    "            eta = eta_values[h][0]\n",
    "            Layer1=Layer(weight_layer_1[f].reshape(2,2), eta, bias)\n",
    "            #print(\"Layer 1 weights: \", Layer1.layer_weight)\n",
    "            Layer2=Layer(weight_layer_2[g].reshape(2,1), eta, bias)\n",
    "            #print(\"Layer 1 weights: \", Layer1.layer_weight)\n",
    "            \n",
    "            #train the network for specified number of iterations\n",
    "            for i in range(0,iterations):\n",
    "                total_error = 0\n",
    "                #Train network for method 1:\n",
    "                for j in range(0,len(ar_train_x)):\n",
    "                    #Input1\n",
    "                    #print(\"For input pair: \", j, \" ,\", ar_train_x[j])\n",
    "                    Layer1.get_layer_output_vector(ar_train_x[j])\n",
    "                    #print(\"Layer1  layer weight is: \", Layer1.layer_weight)\n",
    "                    #print(\"Layer1 layer bias is: \", Layer1.layer_bias)\n",
    "                    #print(\"Layer 1 activity vector is: \", Layer1.activity)\n",
    "                    #print(\"Layer 1 output vector is: \", Layer1.output_vector)\n",
    "                    Layer2.get_layer_output_vector(Layer1.output_vector)\n",
    "                    #print(\"Layer2  layer weight is: \", Layer2.layer_weight)\n",
    "                    #print(\"Layer2 layer bias is: \", Layer2.layer_bias)\n",
    "                    #print(\"Layer2 activity vector is: \", Layer2.activity)\n",
    "                    #print(\"Layer2 output vector is: \", Layer2.output_vector)\n",
    "                    #print(\"Input 1, Iteration: \", i, \" Layer output \", Layer2.output_vector)\n",
    "                    Layer2.calc_delta_k(ar_train_y[j])\n",
    "                    #print(\"Layer2 little E vector is: \", Layer2.littleE)\n",
    "                    #print(\"Layer2 Big E vector is: \", Layer2.bigE)\n",
    "                    #print(\"Layer2 delta k vector is: \", Layer2.delta_k)\n",
    "                    Layer2.calc_delta_weights(Layer2.delta_k, Layer1.output_vector.reshape(2,1))\n",
    "                    #print(\"Layer 2 delta weights :\", Layer2.delta_weights)\n",
    "                    #print(\"layer 2 delta bias: \", Layer2.delta_bias)\n",
    "                    Layer1.calc_delta_j(Layer2.delta_k, Layer2.layer_weight)\n",
    "                    #print(\"Layer1 delta j: \", Layer1.delta_j)\n",
    "                    Layer1.calc_delta_weights(Layer1.delta_j, ar_train_x[j])\n",
    "                    #print(\"Layer 1 delta weights :\", Layer1.delta_weights)\n",
    "                    #print(\"layer 1 delta bias: \", Layer1.delta_bias)\n",
    "                    Layer1.update_layer_weights()\n",
    "                    #print(\"Layer 1 updated weights :\", Layer1.layer_weight)\n",
    "                    #print(\"layer 1 updated bias: \", Layer1.layer_bias)\n",
    "                    Layer2.update_layer_weights()\n",
    "                    #print(\"Layer 2 updated weights :\", Layer2.layer_weight)\n",
    "                    #print(\"layer 2 updated bias: \", Layer2.layer_bias)\n",
    "                    total_error = total_error + Layer2.bigE\n",
    "                    #print(\"Total error for iteration \", i, \" and input pair \", j, \" is \", Layer2.bigE)\n",
    "                    #print(\"------Break-------\")\n",
    "\n",
    "                if(abs(total_error) < lowest_E):\n",
    "                    lowest_E=total_error\n",
    "                    best_weights1 = weight_layer_1[f]\n",
    "                    best_weights2 = weight_layer_2[g]\n",
    "                    best_eta = eta\n",
    "                \n",
    "print(\"Best Weight for layer 1: \", best_weights1, \"Best weights for layer 2:\", best_weights2,  \" Best eta: \", best_eta, \"Lowest E: \", lowest_E, \", when averaged is \",lowest_E/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c3451",
   "metadata": {},
   "source": [
    "# Initialize Multi-Layer Network with Best Weights/Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46f7fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set weights and eta to best model\n",
    "eta = best_eta\n",
    "weight_vector_1 = best_weights1\n",
    "weight_vector_2 = best_weights2\n",
    "bias = 0\n",
    "\n",
    "Layer1=Layer(weight_vector_1.reshape(2,2), eta, bias)\n",
    "Layer2=Layer(weight_vector_2.reshape(2,1), eta, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36464132",
   "metadata": {},
   "source": [
    "## Retrain Network using Method 1 (30 iters, sequential I/O pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12349c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the network with the best eta and weight values\n",
    "for i in range(0,iterations):\n",
    "    #Train network for method 1:\n",
    "    for j in range(0,len(ar_train_x)):\n",
    "        Layer1.get_layer_output_vector(ar_train_x[j])\n",
    "        Layer2.get_layer_output_vector(Layer1.output_vector)\n",
    "        Layer2.calc_delta_k(ar_train_y[j])\n",
    "        Layer2.calc_delta_weights(Layer2.delta_k, Layer1.output_vector.reshape(2,1))\n",
    "        Layer1.calc_delta_j(Layer2.delta_k, Layer2.layer_weight)\n",
    "        Layer1.calc_delta_weights(Layer1.delta_j, ar_train_x[j])\n",
    "        Layer1.update_layer_weights()\n",
    "        Layer2.update_layer_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d2e30",
   "metadata": {},
   "source": [
    "## Using trained model, make predictions on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "084b14ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [[0.556]]  Actual: 1.0\n",
      "Predicted:  [[0.502]]  Actual: 1.0\n",
      "Predicted:  [[0.353]]  Actual: 0.0\n",
      "Predicted:  [[0.467]]  Actual: 0.0\n",
      "Predicted:  [[0.392]]  Actual: 0.0\n",
      "Predicted:  [[0.317]]  Actual: 1.0\n",
      "Predicted:  [[0.868]]  Actual: 1.0\n",
      "Predicted:  [[0.37]]  Actual: 0.0\n",
      "Predicted:  [[0.664]]  Actual: 1.0\n",
      "Predicted:  [[0.341]]  Actual: 0.0\n"
     ]
    }
   ],
   "source": [
    "#With trained model, use training value predictions to find optimal threshold:\n",
    "predictions = []\n",
    "for j in range (0, len(ar_train_x)): \n",
    "    Layer1.get_layer_output_vector(ar_train_x[j])\n",
    "    Layer2.get_layer_output_vector(Layer1.output_vector)\n",
    "    pred = Layer2.output_vector\n",
    "    predictions.append(pred)\n",
    "    print(\"Predicted: \", np.round(pred,3), \" Actual:\", ar_train_y[j]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1c3a6",
   "metadata": {},
   "source": [
    "## Find optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e6fa89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 90.0  when threshold is:  0.48\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0\n",
    "best_accuracy = 0\n",
    "best_threshold = 0.0\n",
    "\n",
    "while threshold < 1.0:\n",
    "    mapped_predictions = [0 if val < threshold else 1 for val in predictions]\n",
    "    \n",
    "    \n",
    "    # First, calculate the number of correct predictions\n",
    "    correct_predictions = sum([1 for a, b in zip(ar_train_y, mapped_predictions) if a == b])\n",
    "\n",
    "    # Then, calculate the accuracy as a percentage of correct predictions\n",
    "    accuracy = correct_predictions / len(ar_train_y) * 100\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "        \n",
    "    threshold = threshold + 0.02\n",
    "\n",
    "print(\"Best Accuracy:\", best_accuracy, \" when threshold is: \", round(best_threshold,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c0dcf",
   "metadata": {},
   "source": [
    "## Apply threshold and evaluate against Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc464c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "#With trained model, use test values to evaluate\n",
    "for j in range (0, len(ar_test_x)): \n",
    "    Layer1.get_layer_output_vector(ar_test_x[j])\n",
    "    Layer2.get_layer_output_vector(Layer1.output_vector)\n",
    "    pred = Layer2.output_vector\n",
    "    predictions.append(pred)\n",
    "    \n",
    "mapped_predictions = [0 if val < best_threshold else 1 for val in predictions]\n",
    "correct_predictions = sum([1 for a, b in zip(ar_test_y, mapped_predictions) if a == b])\n",
    "accuracy = correct_predictions / len(ar_test_y) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a75ed9",
   "metadata": {},
   "source": [
    "## A threshold of 0.48 would correctly match 80% of Test and 90% Train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b4a7e",
   "metadata": {},
   "source": [
    "# BREAK  - Still working through what we would need to visualize.  ROCs, Specificity vs Sensitivity, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
